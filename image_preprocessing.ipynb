{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zV6DDe6JvojQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "dPG1gC8Nvjep",
    "outputId": "bfb682be-3c8c-4967-92bd-b02f9a337f8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Libraries -\n",
    "# General:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import sqrt, ceil, floor\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Keras:\n",
    "import keras\n",
    "from keras import Model, Sequential, layers, models, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import AveragePooling2D, AvgPool2D, Conv2D, MaxPool2D, GlobalAvgPool2D, Dropout, Dense, Flatten\n",
    "\n",
    "# Sklearn:\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import tree, svm, linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "# Visualization:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_palette('Blues')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zVCKyKoy5Ma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run to use in Google Colab:\n",
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXaKJ3sqy5Jg"
   },
   "outputs": [],
   "source": [
    "# Preparing datasets for further using\n",
    "# Loading Traffic Signs and plotting all classes with their labels\n",
    "# Plotting histogram with number of images for every class\n",
    "# Equalizing training dataset making examples in the classes equal\n",
    "# Preprocessing datasets\n",
    "#   data0.pickle - Shuffling\n",
    "#   data1.pickle - Shuffling, /255.0 Normalization\n",
    "#   data2.pickle - Shuffling, /255.0 + Mean Normalization\n",
    "#   data3.pickle - Shuffling, /255.0 + Mean + STD Normalization\n",
    "#   data4.pickle - Grayscale, Shuffling\n",
    "#   data5.pickle - Grayscale, Shuffling, Local Histogram Equalization\n",
    "#   data6.pickle - Grayscale, Shuffling, Local Histogram Equalization, /255.0 Normalization\n",
    "#   data7.pickle - Grayscale, Shuffling, Local Histogram Equalization, /255.0 + Mean Normalization\n",
    "#   data8.pickle - Grayscale, Shuffling, Local Histogram Equalization, /255.0 + Mean + STD Normalization\n",
    "# Saving preprocessed datasets into files\n",
    "\n",
    "# Source in hyperlink below\n",
    "\n",
    "\n",
    "\"\"\"Importing library for object serialization\n",
    "which we'll use for saving and loading serialized models\"\"\"\n",
    "import pickle\n",
    "\n",
    "# Importing other standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pylab import text\n",
    "import csv\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "# Defining function for loading dataset from 'pickle' file\n",
    "def load_rgb_data(file):\n",
    "    # Opening 'pickle' file and getting images\n",
    "    with open(file, 'rb') as f:\n",
    "        d = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "        # At the same time method 'astype()' used for converting ndarray from int to float\n",
    "        # It is needed to divide float by float when applying Normalization\n",
    "        x = d['features'].astype(np.float32)   # 4D numpy.ndarray type, for train = (34799, 32, 32, 3)\n",
    "        y = d['labels']                        # 1D numpy.ndarray type, for train = (34799,)\n",
    "        s = d['sizes']                         # 2D numpy.ndarray type, for train = (34799, 2)\n",
    "        c = d['coords']                        # 2D numpy.ndarray type, for train = (34799, 4)\n",
    "        \"\"\"\n",
    "        Data is a dictionary with four keys:\n",
    "            'features' - is a 4D array with raw pixel data of the traffic sign images,\n",
    "                         (number of examples, width, height, channels).\n",
    "            'labels'   - is a 1D array containing the label id of the traffic sign image,\n",
    "                         file label_names.csv contains id -> name mappings.\n",
    "            'sizes'    - is a 2D array containing arrays (width, height),\n",
    "                         representing the original width and height of the image.\n",
    "            'coords'   - is a 2D array containing arrays (x1, y1, x2, y2),\n",
    "                         representing coordinates of a bounding frame around the image.\n",
    "        \"\"\"\n",
    "\n",
    "    # Returning ready data\n",
    "    return x, y, s, c\n",
    "\n",
    "\n",
    "# Defining function for converting data to grayscale\n",
    "def rgb_to_gray_data(x_data):\n",
    "    # Preparing zero valued array for storing GrayScale images with only one channel\n",
    "    x_g = np.zeros((x_data.shape[0], 1, 32, 32))\n",
    "\n",
    "    # Converting RGB images into GrayScale images\n",
    "    # Using formula:\n",
    "    # Y' = 0.299 R + 0.587 G + 0.114 B\n",
    "    x_g[:, 0, :, :] = x_data[:, 0, :, :] * 0.299 + x_data[:, 1, :, :] * 0.587 + x_data[:, 2, :, :] * 0.114\n",
    "\n",
    "    # Also, possible to do with OpenCV\n",
    "    # cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Returning ready data\n",
    "    return x_g\n",
    "\n",
    "\n",
    "# Defining function for getting texts for every class - labels\n",
    "def label_text(file):\n",
    "    # Defining list for saving label in order from 0 to 42\n",
    "    label_list = []\n",
    "\n",
    "    # Opening 'csv' file and getting image's labels\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # Going through all rows\n",
    "        for row in reader:\n",
    "            # Adding from every row second column with name of the label\n",
    "            label_list.append(row[1])\n",
    "        # Deleting the first element of list because it is the name of the column\n",
    "        del label_list[0]\n",
    "    # Returning resulted list\n",
    "    return label_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "https://www.rapidtables.com/convert/color/rgb-to-hsv.html\n",
    "https://ru.wikipedia.org/wiki/HSV_(цветовая_модель)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Defining function for changing brightness\n",
    "def brightness_changing(image):\n",
    "    # Converting firstly image from RGB to HSV\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # Defining random value for changing brightness\n",
    "    random_brightness = 0.25 + np.random.uniform()\n",
    "    # Implementing changing of Value channel of HSV image\n",
    "    image_hsv[:, :, 2] = image_hsv[:, :, 2] * random_brightness\n",
    "    # Converting HSV changed image to RGB\n",
    "    image_rgb = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2RGB)\n",
    "    # Returning image with changed brightness\n",
    "    return image_rgb\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "To rotate an image using OpenCV Python,\n",
    "first, calculate the affine matrix that does the affine transformation (linear mapping of pixels),\n",
    "then warp the input image with the affine matrix.\n",
    "\n",
    "Example:\n",
    "\n",
    "M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "rotated = cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "where\n",
    "\n",
    "center:  center of the image (the point about which rotation has to happen)\n",
    "angle:   angle by which image has to be rotated in the anti-clockwise direction\n",
    "scale:   1.0 mean, the shape is preserved. Other value scales the image by the value provided\n",
    "rotated: ndarray that holds the rotated image data\n",
    "\n",
    "Note: Observe that the dimensions of the resulting image are provided same as that of the original image.\n",
    "When we are rotating by 90 or 270 and would to affect the height and width as well,\n",
    "swap height with width and width with height.\n",
    "\n",
    "https://www.tutorialkart.com/opencv/python/opencv-python-rotate-image/\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Defining function for changing rotation of image\n",
    "def rotation_changing(image):\n",
    "    # Defining angle range\n",
    "    angle_range = 25\n",
    "    # Defining angle rotation\n",
    "    angle_rotation = np.random.uniform(angle_range) - angle_range / 2\n",
    "    # Getting shape of image\n",
    "    rows, columns, channels = image.shape\n",
    "    # Implementing rotation\n",
    "    # Calculating Affine Matrix\n",
    "    affine_matrix = cv2.getRotationMatrix2D((columns / 2, rows / 2), angle_rotation, 1)\n",
    "    # Warping original image with Affine Matrix\n",
    "    rotated_image = cv2.warpAffine(image, affine_matrix, (columns, rows))\n",
    "    # Returning rotated image\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "# Defining function for transformation: brightness + rotation\n",
    "def transformation_brightness_rotation(image):\n",
    "    return brightness_changing(rotation_changing(image))\n",
    "\n",
    "\n",
    "# Defining function for getting random image of one label\n",
    "def random_image(x_train, y_train, y_number):\n",
    "    # Getting indexes of needed 'y_number' from 'y_train'\n",
    "    # Defining True - False array\n",
    "    image_indexes = np.where(y_train == y_number)\n",
    "    # Getting random index of needed label\n",
    "    # 'np.bincount(y_train)' - array with number of examples for every label\n",
    "    # 'np.bincount(y_train)[y_number] - 1' - number of examples for 'y_number' label\n",
    "    random_index = np.random.randint(0, np.bincount(y_train)[y_number] - 1)\n",
    "    # Returning random image from 'x_train'\n",
    "    # 'x_train[image_indexes]' - returns array with only 'y_number' label\n",
    "    # 'x_train[image_indexes][random_index]' - random image of needed label\n",
    "    return x_train[image_indexes][random_index]\n",
    "\n",
    "\n",
    "# Defining function for equalization training dataset\n",
    "def equalize_training_dataset(x_train, y_train):\n",
    "    # Getting number of examples for every label\n",
    "    number_of_examples_for_every_label = np.bincount(y_train)\n",
    "    # Calculating total amount of unique labels\n",
    "    number_of_labels = np.arange(len(number_of_examples_for_every_label))\n",
    "\n",
    "    # Iterating over all number of labels\n",
    "    # Showing progress ber with 'tqdm'\n",
    "    for i in tqdm(number_of_labels):\n",
    "        # Calculating how many examples is needed to add for current label\n",
    "        # 'np.mean(number_of_examples_for_every_label)' - average number over examples for every label\n",
    "        number_of_examples_to_add = int(np.mean(number_of_examples_for_every_label) * 2.5) - \\\n",
    "                                    number_of_examples_for_every_label[i]\n",
    "\n",
    "        # Defining temporary arrays for collecting new images\n",
    "        x_temp = []\n",
    "        y_temp = []\n",
    "\n",
    "        # Getting random image from current label\n",
    "        # Transforming it and adding to the temporary arrays\n",
    "        for j in range(number_of_examples_to_add):\n",
    "            getting_random_image = random_image(x_train, y_train, i)\n",
    "            x_temp.append(transformation_brightness_rotation(getting_random_image))\n",
    "            y_temp.append(i)\n",
    "\n",
    "        x_train = np.append(x_train, np.array(x_temp), axis=0)\n",
    "        y_train = np.append(y_train, np.array(y_temp), axis=0)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "# Defining function for Local Histogram Equalization\n",
    "def local_histogram_equalization(image):\n",
    "    # Creating CLAHE object with arguments\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "\n",
    "    # Applying Local Histogram Equalization and returning resulted image\n",
    "    return clahe.apply(image)\n",
    "\n",
    "\n",
    "# Defining function for preprocessing loaded data\n",
    "def preprocess_data(d, shuffle=False, lhe=False, norm_255=False, mean_norm=False, std_norm=False,\n",
    "                    transpose=True, colour='rgb'):\n",
    "    # Applying Shuffling\n",
    "    if shuffle:\n",
    "        # Shuffle data\n",
    "        # Multi-dimensional arrays are only shuffled along the first axis\n",
    "        # By using seed we generate two times the same random numbers\n",
    "        # And save appropriate connection: image --> label\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_train'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_train'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_validation'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_validation'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_test'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_test'])\n",
    "        # Also, possible to do like following:\n",
    "        # x_train, y_train = shuffle(x_train, y_train)\n",
    "        # This function is from sklearn library:\n",
    "        # from sklearn.utils import shuffle\n",
    "\n",
    "    # Applying Local Histogram Equalization\n",
    "    if lhe:\n",
    "        # Function map applies first argument to all elements of the second argument\n",
    "        # First argument in our case is a function\n",
    "        # Second argument in our case is np array\n",
    "        # We need to slice it in order to pass into the function only (32, 32) and not (1, 32, 32)\n",
    "        # Also, map functions applies to first argument all images of the second argument\n",
    "        # In our case it is a number of d['x_train'].shape[0]\n",
    "        # Result we wrap with list and then list convert to np.array\n",
    "        # And reshaping it to make it again 4D tensor\n",
    "\n",
    "        d['x_train'] = list(map(local_histogram_equalization, d['x_train'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_train'] = np.array(d['x_train'])\n",
    "        d['x_train'] = d['x_train'].reshape(d['x_train'].shape[0], 1, 32, 32)\n",
    "        d['x_train'] = d['x_train'].astype(np.float32)\n",
    "        d['x_validation'] = list(map(local_histogram_equalization, d['x_validation'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_validation'] = np.array(d['x_validation'])\n",
    "        d['x_validation'] = d['x_validation'].reshape(d['x_validation'].shape[0], 1, 32, 32)\n",
    "        d['x_validation'] = d['x_validation'].astype(np.float32)\n",
    "        d['x_test'] = list(map(local_histogram_equalization, d['x_test'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_test'] = np.array(d['x_test'])\n",
    "        d['x_test'] = d['x_test'].reshape(d['x_test'].shape[0], 1, 32, 32)\n",
    "        d['x_test'] = d['x_test'].astype(np.float32)\n",
    "\n",
    "    # Applying /255.0 Normalization\n",
    "    if norm_255:\n",
    "        # Normalizing whole data by dividing /255.0\n",
    "        d['x_train'] = d['x_train'].astype(np.float32) / 255.0\n",
    "        d['x_validation'] /= 255.0\n",
    "        d['x_test'] /= 255.0\n",
    "\n",
    "        # Preparing 'mean image'\n",
    "        # Subtracting the dataset by 'mean image' serves to center the data\n",
    "        # It helps for each feature to have a similar range and gradients don't go out of control.\n",
    "        # Calculating 'mean image' from training dataset along the rows by specifying 'axis=0'\n",
    "        # We CALCULATE 'mean image' ONLY FROM TRAINING dataset\n",
    "        # Calculating mean image from training dataset along the rows by specifying 'axis=0'\n",
    "        mean_image = np.mean(d['x_train'], axis=0)  # numpy.ndarray (3, 32, 32)\n",
    "        # Saving calculated 'mean_image' into 'pickle' file\n",
    "        # We will use it when preprocess input data for classifying\n",
    "        # We will need to subtract input image for classifying\n",
    "        # As we're doing now for training, validation and testing data\n",
    "        dictionary = {'mean_image_' + colour: mean_image}\n",
    "        with open('mean_image_' + colour + '.pickle', 'wb') as f_mean_image:\n",
    "            pickle.dump(dictionary, f_mean_image)\n",
    "\n",
    "        # Preparing 'std image'\n",
    "        # Calculating standard deviation from training dataset along the rows by specifying 'axis=0'\n",
    "        std = np.std(d['x_train'], axis=0)  # numpy.ndarray (3, 32, 32)\n",
    "        # Saving calculated 'std' into 'pickle' file\n",
    "        # We will use it when preprocess input data for classifying\n",
    "        # We will need to divide input image for classifying\n",
    "        # As we're doing now for training, validation and testing data\n",
    "        dictionary = {'std_' + colour: std}\n",
    "        with open('std_' + colour + '.pickle', 'wb') as f_std:\n",
    "            pickle.dump(dictionary, f_std)\n",
    "\n",
    "    # Applying Mean Normalization\n",
    "    if mean_norm:\n",
    "        # Normalizing data by subtracting with 'mean image'\n",
    "        # Getting saved data for 'mean image'\n",
    "        # Opening file for reading in binary mode\n",
    "        with open('mean_image_' + colour + '.pickle', 'rb') as f:\n",
    "            mean_image = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "\n",
    "        d['x_train'] -= mean_image['mean_image_' + colour]\n",
    "        d['x_validation'] -= mean_image['mean_image_' + colour]\n",
    "        d['x_test'] -= mean_image['mean_image_' + colour]\n",
    "\n",
    "    # Applying STD Normalization\n",
    "    if std_norm:\n",
    "        # Normalizing data by dividing with 'standard deviation'\n",
    "        # Getting saved data for 'std image'\n",
    "        # Opening file for reading in binary mode\n",
    "        with open('std_' + colour + '.pickle', 'rb') as f:\n",
    "            std = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "\n",
    "        # Don't forget to change names for mean and std files when preprocessing for grayscale purposes\n",
    "        d['x_train'] /= std['std_' + colour]\n",
    "        d['x_validation'] /= std['std_' + colour]\n",
    "        d['x_test'] /= std['std_' + colour]\n",
    "\n",
    "    # WARNING!\n",
    "    # Do not make transpose starting from data1\n",
    "    # As data0 was already transposed\n",
    "    if transpose:\n",
    "        # Transposing every dataset to make channels come first\n",
    "        d['x_train'] = d['x_train'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "        d['x_validation'] = d['x_validation'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "        d['x_test'] = d['x_test'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "\n",
    "    # Returning preprocessed data\n",
    "    return d\n",
    "\n",
    "\n",
    "# WARNING! Load and preprocess data for rgb and grayscale separately\n",
    "\n",
    "\n",
    "# <---------->\n",
    "# Option 1 - rgb data --> starts here\n",
    "#\n",
    "# # Loading rgb data from training dataset\n",
    "# x_train, y_train, s_train, c_train = load_rgb_data('train.pickle')\n",
    "#\n",
    "# # Loading rgb data from validation dataset\n",
    "# x_validation, y_validation, s_validation, c_validation = load_rgb_data('valid.pickle')\n",
    "#\n",
    "# # Loading rgb data from test dataset\n",
    "# x_test, y_test, s_test, c_test = load_rgb_data('test.pickle')\n",
    "#\n",
    "# # Getting texts for every class\n",
    "# label_list = label_text('label_names.csv')\n",
    "#\n",
    "# # Plotting 43 unique examples with their label's names\n",
    "# # And histogram of 43 classes with their number of examples\n",
    "# plot_unique_examples(x_train, y_train)\n",
    "#\n",
    "# # Plotting 43 good quality examples to show in GUI for driver\n",
    "# plot_signs()\n",
    "#\n",
    "# # Implementing equalization of training dataset\n",
    "# x_train, y_train = equalize_training_dataset(x_train.astype(np.uint8), y_train)\n",
    "#\n",
    "# # Plotting 43 unique examples with their label's names\n",
    "# # And histogram of 43 classes with their number of examples\n",
    "# plot_unique_examples(x_train, y_train)\n",
    "#\n",
    "# # Putting loaded and equalized data into the dictionary\n",
    "# # Equalization is done only for training dataset\n",
    "# d_loaded = {'x_train': x_train, 'y_train': y_train,\n",
    "#             'x_validation': x_validation, 'y_validation': y_validation,\n",
    "#             'x_test': x_test, 'y_test': y_test,\n",
    "#             'labels': label_list}\n",
    "\n",
    "\n",
    "# WARNING! It is important to run different preprocessing approaches separately\n",
    "# Otherwise, dictionary will change values increasingly\n",
    "# Also, creating separate dictionaries like 'd0, d1, d2, d3' will not help\n",
    "# As they all contain same references to the datasets\n",
    "\n",
    "\n",
    "# # Applying preprocessing\n",
    "# data0 = preprocess_data(d_loaded, shuffle=True, transpose=True)\n",
    "# print('Before Backward Calculation')\n",
    "# print(data0['x_train'][0, 0, :, 0])\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data0.pickle', 'wb') as f:\n",
    "#     pickle.dump(data0, f)\n",
    "# # Releasing memory\n",
    "# del data0\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data0.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data0.pickle', 'rb') as f:\n",
    "#     d_0_1 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data0 --> data1\n",
    "# data1 = preprocess_data(d_0_1, shuffle=False, norm_255=True, transpose=False, colour='rgb')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data1.pickle', 'wb') as f:\n",
    "#     pickle.dump(data1, f)\n",
    "# # Releasing memory\n",
    "# del d_0_1\n",
    "# del data1\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data0.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data0.pickle', 'rb') as f:\n",
    "#     d_0_2 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data0 --> data2\n",
    "# data2 = preprocess_data(d_0_2, shuffle=False, norm_255=True, mean_norm=True, transpose=False,\n",
    "#                         colour='rgb')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data2.pickle', 'wb') as f:\n",
    "#     pickle.dump(data2, f)\n",
    "# # Releasing memory\n",
    "# del d_0_2\n",
    "# del data2\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data0.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data0.pickle', 'rb') as f:\n",
    "#     d_0_3 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data0 --> data3\n",
    "# data3 = preprocess_data(d_0_3, shuffle=False, norm_255=True, mean_norm=True, std_norm=True,\n",
    "#                         transpose=False, colour='rgb')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data3.pickle', 'wb') as f:\n",
    "#     pickle.dump(data3, f)\n",
    "# # Releasing memory\n",
    "# del d_0_3\n",
    "# del data3\n",
    "\n",
    "\n",
    "# # Checking received preprocessed data by doing backward calculations\n",
    "# # Getting mean and std\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('mean_image_rgb.pickle', 'rb') as f:\n",
    "#     mean_image_rgb = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "#\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('std_rgb.pickle', 'rb') as f:\n",
    "#     std_rgb = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "#\n",
    "# # Loading 'data3.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data3.pickle', 'rb') as f:\n",
    "#     data3 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "#\n",
    "# print(data3['x_train'].shape)\n",
    "# # Starting from fully preprocessed dataset\n",
    "# d3 = data3['x_train']\n",
    "# print(d3.shape)  # (86989, 3, 32, 32)\n",
    "#\n",
    "# # Multiplying by std\n",
    "# d2 = d3 * std_rgb['std_rgb']\n",
    "# print(d2.shape)  # (86989, 3, 32, 32)\n",
    "#\n",
    "# # Adding with mean\n",
    "# d1 = d2 + mean_image_rgb['mean_image_rgb']\n",
    "# print(d1.shape)  # (86989, 3, 32, 32)\n",
    "#\n",
    "# # Multiplying by 255.0\n",
    "# d0 = d1 * 255.0\n",
    "# print(d0.shape)  # (86989, 3, 32, 32)\n",
    "#\n",
    "# # Showing result\n",
    "# print('After Backward Calculation')\n",
    "# print(d0[0, 0, :, 0])\n",
    "\n",
    "# <---------->\n",
    "# Option 1 - rgb data --> ends here\n",
    "\n",
    "\n",
    "# <---------->\n",
    "# Option 2 - grayscale data --> starts here\n",
    "\n",
    "# # Loading 'data0.pickle' rgb dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data0.pickle', 'rb') as f:\n",
    "#     data0 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "#\n",
    "# # Converting rgb data to grayscale for training dataset\n",
    "# x_train = rgb_to_gray_data(data0['x_train'])\n",
    "#\n",
    "# # Converting rgb data to grayscale for validation dataset\n",
    "# x_validation = rgb_to_gray_data(data0['x_validation'])\n",
    "#\n",
    "# # Converting rgb data to grayscale for testing dataset\n",
    "# x_test = rgb_to_gray_data(data0['x_test'])\n",
    "#\n",
    "# # Putting loaded data into the dictionary\n",
    "# d_loaded_gray = {'x_train': x_train, 'y_train': data0['y_train'],\n",
    "#                  'x_validation': x_validation, 'y_validation': data0['y_validation'],\n",
    "#                  'x_test': x_test, 'y_test': data0['y_test'],\n",
    "#                  'labels': data0['labels']}\n",
    "#\n",
    "# # Showing the image by using obtained array with only one channel\n",
    "# # Pay attention that when we use only one channeled array of image\n",
    "# # We need to use (32, 32) and not (32, 32, 1) to show with 'plt.imshow'\n",
    "# plt.imshow(x_train[9000, 0, :, :].astype(np.uint8), cmap=plt.get_cmap('gray'))\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# WARNING! It is important to run different preprocessing approaches separately\n",
    "# Otherwise, dictionary will change values increasingly\n",
    "# Also, creating separate dictionaries like 'd0, d1, d2, d3' will not help\n",
    "# As they all contain same references to the datasets\n",
    "\n",
    "\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data4.pickle', 'wb') as f:\n",
    "#     pickle.dump(d_loaded_gray, f)\n",
    "# # Releasing memory\n",
    "# del d_loaded_gray\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data4.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data4.pickle', 'rb') as f:\n",
    "#     d_4_5 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data4 --> data5\n",
    "# data5 = preprocess_data(d_4_5, shuffle=False, lhe=True, transpose=False, colour='gray')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# print('Before Backward Calculation')\n",
    "# print(data5['x_train'][0, 0, :, 0])\n",
    "# with open('data5.pickle', 'wb') as f:\n",
    "#     pickle.dump(data5, f)\n",
    "# # Releasing memory\n",
    "# del d_4_5\n",
    "# del data5\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data4.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data4.pickle', 'rb') as f:\n",
    "#     d_4_6 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data4 --> data6\n",
    "# data6 = preprocess_data(d_4_6, shuffle=False, lhe=True, norm_255=True, transpose=False,\n",
    "#                         colour='gray')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data6.pickle', 'wb') as f:\n",
    "#     pickle.dump(data6, f)\n",
    "# # Releasing memory\n",
    "# del d_4_6\n",
    "# del data6\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data4.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data4.pickle', 'rb') as f:\n",
    "#     d_4_7 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data4 --> data7\n",
    "# data7 = preprocess_data(d_4_7, shuffle=False, lhe=True, norm_255=True, mean_norm=True,\n",
    "#                         transpose=False, colour='gray')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data7.pickle', 'wb') as f:\n",
    "#     pickle.dump(data7, f)\n",
    "# # Releasing memory\n",
    "# del d_4_7\n",
    "# del data7\n",
    "\n",
    "# # Applying preprocessing\n",
    "# # Loading 'data4.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data4.pickle', 'rb') as f:\n",
    "#     d_4_8 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "# # Preprocessing data4 --> data8\n",
    "# data8 = preprocess_data(d_4_8, shuffle=False, lhe=True, norm_255=True, mean_norm=True, std_norm=True,\n",
    "#                         transpose=False, colour='gray')\n",
    "# # Saving loaded and preprocessed data into 'pickle' file\n",
    "# with open('data8.pickle', 'wb') as f:\n",
    "#     pickle.dump(data8, f)\n",
    "# # Releasing memory\n",
    "# del d_4_8\n",
    "# del data8\n",
    "\n",
    "\n",
    "# # Checking received preprocessed data by doing backward calculations\n",
    "# # Getting mean and std\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('mean_image_gray.pickle', 'rb') as f:\n",
    "#     mean_image_gray = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "#\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('std_gray.pickle', 'rb') as f:\n",
    "#     std_gray = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "#\n",
    "# # Loading 'data8.pickle' dataset and going further with it\n",
    "# # Opening file for reading in binary mode\n",
    "# with open('data8.pickle', 'rb') as f:\n",
    "#     data8 = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "#\n",
    "# # Starting from fully preprocessed dataset\n",
    "# d8 = data8['x_train']\n",
    "# print(d8.shape)  # (86989, 1, 32, 32)\n",
    "#\n",
    "# # Multiplying by std\n",
    "# d7 = d8 * std_gray['std_gray']\n",
    "# print(d7.shape)  # (86989, 1, 32, 32)\n",
    "#\n",
    "# # Adding with mean\n",
    "# d6 = d7 + mean_image_gray['mean_image_gray']\n",
    "# print(d6.shape)  # (86989, 1, 32, 32)\n",
    "#\n",
    "# # Multiplying by 255.0\n",
    "# d5 = d6 * 255.0\n",
    "# print(d5.shape)  # (86989, 1, 32, 32)\n",
    "#\n",
    "# # Showing result\n",
    "# print('After Backward Calculation')\n",
    "# print(d5[0, 0, :, 0])\n",
    "\n",
    "# <---------->\n",
    "# Option 2 - grayscale data --> ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-uFJbXnzmoz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/test.pickle\n",
      "data/test.pickle.zip\n",
      "data/data1.pickle.zip\n",
      "data/train.pickle.zip\n",
      "data/data2.pickle\n",
      "data/data4.pickle.zip\n",
      "data/std_gray.pickle\n",
      "data/mean_image_rgb.pickle\n",
      "data/data_dict_2.pickle\n",
      "data/data6.pickle\n",
      "data/datasets_preparing.py\n",
      "data/data3.pickle.zip\n",
      "data/data6.pickle.zip\n",
      "data/data8.pickle\n",
      "data/data4.pickle\n",
      "data/label_names.csv\n",
      "data/data5.pickle.zip\n",
      "data/data1.pickle\n",
      "data/valid.pickle\n",
      "data/data8.pickle.zip\n",
      "data/std_rgb.pickle\n",
      "data/data0.pickle.zip\n",
      "data/valid.pickle.zip\n",
      "data/data3.pickle\n",
      "data/data7.pickle.zip\n",
      "data/train.pickle\n",
      "data/data7.pickle\n",
      "data/mean_image_gray.pickle\n",
      "data/data5.pickle\n",
      "data/labels.pickle\n",
      "data/data2.pickle.zip\n",
      "['test.pickle', 'test.pickle.zip', 'data1.pickle.zip', 'train.pickle.zip', 'data2.pickle', 'data4.pickle.zip', 'std_gray.pickle', 'mean_image_rgb.pickle', 'data_dict_2.pickle', 'data6.pickle', 'datasets_preparing.py', 'data3.pickle.zip', 'data6.pickle.zip', 'data8.pickle', 'data4.pickle', 'label_names.csv', 'data5.pickle.zip', 'data1.pickle', 'valid.pickle', 'data8.pickle.zip', 'std_rgb.pickle', 'data0.pickle.zip', 'valid.pickle.zip', 'data3.pickle', 'data7.pickle.zip', 'train.pickle', 'data7.pickle', 'mean_image_gray.pickle', 'data5.pickle', 'labels.pickle', 'data2.pickle.zip']\n"
     ]
    }
   ],
   "source": [
    "# Don't run this block in colab\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "print(os.listdir('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TQv2Gehy5GW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: (12630,)\n",
      "y_validation: (4410, 43)\n",
      "x_validation: (4410, 32, 32, 3)\n",
      "x_train: (86989, 32, 32, 3)\n",
      "y_train: (86989, 43)\n",
      "labels: 43\n",
      "x_test: (12630, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Only for Collab:\n",
    "\n",
    "# Opening file for reading in binary mode\n",
    "with open('data/data2.pickle', 'rb') as f:\n",
    "    data = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "\n",
    "# Preparing y_train and y_validation for using in Keras\n",
    "data['y_train'] = to_categorical(data['y_train'], num_classes=43)\n",
    "data['y_validation'] = to_categorical(data['y_validation'], num_classes=43)\n",
    "\n",
    "# Making channels come at the end\n",
    "data['x_train'] = data['x_train'].transpose(0, 2, 3, 1)\n",
    "data['x_validation'] = data['x_validation'].transpose(0, 2, 3, 1)\n",
    "data['x_test'] = data['x_test'].transpose(0, 2, 3, 1)\n",
    "\n",
    "# Showing loaded data from file\n",
    "for i, j in data.items():\n",
    "    if i == 'labels':\n",
    "        print(i + ':', len(j))\n",
    "    else: \n",
    "        print(i + ':', j.shape)\n",
    "\n",
    "# x_train: (86989, 32, 32, 3)\n",
    "# y_train: (86989, 43)\n",
    "# x_test: (12630, 32, 32, 3)\n",
    "# y_test: (12630,)\n",
    "# x_validation: (4410, 32, 32, 3)\n",
    "# y_validation: (4410, 43)\n",
    "# labels: 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvuhL2-F3YeL"
   },
   "outputs": [],
   "source": [
    "pickle_out = open(\"data/data_dict_2.pickle\",\"wb\")\n",
    "pickle.dump(data, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "#/content/drive/My Drive/capstone/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OlzYl-Uy45q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "image_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
